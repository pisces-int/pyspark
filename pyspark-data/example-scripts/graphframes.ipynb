{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bbe9a6-798c-4300-9ed7-6038d7f56904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the shell escape command to install the package\n",
    "!pip install graphframes\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94afeb2-ff28-4bda-8032-2657b695f4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, explode, col, lit,  date_format, to_date, hour, minute, when, sum\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType, BooleanType\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.sql import functions as F\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access sensitive variables\n",
    "es_url = os.getenv(\"ELASTICSEARCH\")\n",
    "es_username = os.getenv(\"ELASTICSEARCH_USERNAME\")\n",
    "es_password = os.getenv(\"ELASTICSEARCH_PASSWORD\")\n",
    "\n",
    "spark = SparkSession.builder.appName(\"REST_API_with_PySpark_DF\") \\\n",
    "    .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.3-spark3.5-s_2.12\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Important parameters can be found here\n",
    "# Some have to be manually modified within the code\n",
    "schema = StructType([\n",
    "    StructField(\"@timestamp\", StringType(), True),\n",
    "    StructField(\"source.address\", StringType(), True),\n",
    "    StructField(\"destination.address\", StringType(), True),\n",
    "    StructField(\"network.bytes\", StringType(), True),\n",
    "])\n",
    "days_to_fetch = 1  # Specify the number of days to fetch\n",
    "\n",
    "@udf(returnType=ArrayType(schema))\n",
    "def fetch_data(offset: int, limit: int, days: int):\n",
    "    endpoint = \"https://\"+es_url+\"/filebeat-*/_search\"\n",
    "\n",
    "    # Calculate start date based on the specified number of days\n",
    "    import datetime\n",
    "    start_date = (datetime.datetime.now() - datetime.timedelta(days=days)).isoformat()\n",
    "\n",
    "    fields = [field.name for field in schema.fields]\n",
    "\n",
    "    # Elasticsearch query with time filter and proper pagination\n",
    "    es_query = {\n",
    "        \"size\": limit,\n",
    "        \"from\": offset,\n",
    "        \"_source\": fields,  # Select only specified fields\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\n",
    "                        \"term\": {\n",
    "                            \"suricata.eve.event_type\": \"flow\"\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"range\": {\n",
    "                            \"@timestamp\": {\n",
    "                                \"gte\": start_date\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(endpoint, json=es_query, headers=headers, verify=False, auth=(es_username, es_password))\n",
    "\n",
    "    # Extract hits from the response\n",
    "    hits = response.json().get('hits', {}).get('hits', [])\n",
    "\n",
    "    # Extract necessary fields from hits and create a list of records\n",
    "    records = [{\"@timestamp\": hit.get('_source', {}).get('@timestamp'),\n",
    "                \"source.address\": hit.get('_source', {}).get('source', {}).get('address'),\n",
    "                \"destination.address\": hit.get('_source', {}).get('destination', {}).get('address'),\n",
    "                \"network.bytes\": hit.get('_source', {}).get('network', {}).get('bytes')}\n",
    "               for hit in hits]\n",
    "\n",
    "    return records  # assuming API returns a list of records\n",
    "\n",
    "# Get total docs\n",
    "total_records = requests.get(\"https://\"+es_url+\"/filebeat-*/_count\", verify=False, auth=(es_username, es_password)).json().get('count', 0)\n",
    "\n",
    "records_per_page = 500\n",
    "\n",
    "# Create DataFrame with pagination information\n",
    "offsets_df = spark.range(0, total_records, records_per_page).select(col(\"id\").alias(\"offset\"), lit(records_per_page).alias(\"limit\"))\n",
    "\n",
    "# Apply fetch_data UDF to get the response with time filter\n",
    "response_df = offsets_df.withColumn(\"response\", explode(fetch_data(\"offset\", \"limit\", lit(days_to_fetch))))\n",
    "\n",
    "# Uncomment the lines above if you want to further explode and select individual fields\n",
    "# response_df.show(truncate=False)\n",
    "\n",
    "# Extract variables using positional indexing\n",
    "result_df = response_df.select(\n",
    "#    \"offset\",\n",
    "#    \"limit\",\n",
    "    col(\"response\")[\"@timestamp\"].alias(\"@timestamp\"),\n",
    "    col(\"response\")[\"source.address\"].alias(\"source.address\"),\n",
    "    col(\"response\")[\"destination.address\"].alias(\"destination.address\"),\n",
    "    col(\"response\")[\"network.bytes\"].alias(\"network.bytes\")\n",
    ")\n",
    "\n",
    "\n",
    "result_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafbf4f0-d9c5-4038-9cef-05f06ef0ae38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipaddress\n",
    "\n",
    "# Define a function to check if an IP address is in RFC1918 private IP range\n",
    "def is_private_ip(ip):\n",
    "    try:\n",
    "        ip_obj = ipaddress.ip_address(ip)\n",
    "        return ip_obj.is_private or (ip_obj.version == 6 and ip_obj.is_link_local) or (ip_obj.version == 4 and not ip_obj.is_global)\n",
    "    except ValueError:\n",
    "        return False  # Not a valid IP address\n",
    "\n",
    "# Create UDF to check if IP is private\n",
    "is_private_ip_udf = udf(is_private_ip, BooleanType())\n",
    "\n",
    "# Create Boolean columns to check if source and destination IPs are private\n",
    "result_df = result_df.withColumn(\"source_is_private\", is_private_ip_udf(col(\"`source.address`\"))) \\\n",
    "                     .withColumn(\"destination_is_private\", is_private_ip_udf(col(\"`destination.address`\")))\n",
    "\n",
    "# Update labels based on private IPs\n",
    "result_df = result_df.withColumn(\"source.address\", when(~col(\"source_is_private\"), \"Public\").otherwise(col(\"`source.address`\"))) \\\n",
    "                     .withColumn(\"destination.address\", when(~col(\"destination_is_private\"), \"Public\").otherwise(col(\"`destination.address`\")))\n",
    "\n",
    "# Drop the auxiliary columns\n",
    "result_df = result_df.drop(\"source_is_private\", \"destination_is_private\")\n",
    "\n",
    "# Filter out multicast addresses\n",
    "result_df = result_df.filter(~col(\"`source.address`\").rlike(\"^224\\\\.\") & ~col(\"`destination.address`\").rlike(\"^224\\\\.\"))\n",
    "result_df = result_df.filter(~col(\"`source.address`\").rlike(\"^255.255.255.255\") & ~col(\"`destination.address`\").rlike(\"^255.255.255.255\"))\n",
    "result_df = result_df.filter(~col(\"`source.address`\").rlike(\"^0.0.0.0\") & ~col(\"`destination.address`\").rlike(\"^0.0.0.0\"))\n",
    "result_df = result_df.filter(~col(\"`source.address`\").rlike(\"^ff00::\") & ~col(\"`destination.address`\").rlike(\"^ff00::\"))\n",
    "result_df = result_df.filter(~col(\"`source.address`\").rlike(\"\\\\.255$\") & ~col(\"`destination.address`\").rlike(\"\\\\.255$\"))\n",
    "result_df = result_df.filter(~(col(\"`source.address`\") == col(\"`destination.address`\")))\n",
    "\n",
    "# Filter out records where network.bytes are 0\n",
    "result_df = result_df.filter(col(\"`network.bytes`\") != 0)\n",
    "\n",
    "# Group by source and destination addresses, summing the network bytes\n",
    "aggregated_df = result_df.groupBy(\"`source.address`\", \"`destination.address`\") \\\n",
    "                  .agg(sum(\"`network.bytes`\").alias(\"network.bytes\"))\n",
    "\n",
    "aggregated_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae0b3cd-5961-442f-ad1e-0cfd2910ee87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from graphframes import GraphFrame\n",
    "\n",
    "# Create a GraphFrame\n",
    "edges = aggregated_df.select(\n",
    "    F.col(\"`source.address`\").alias(\"src\"),\n",
    "    F.col(\"`destination.address`\").alias(\"dst\"),\n",
    "    F.col(\"`network.bytes`\").alias(\"weight\")\n",
    ")\n",
    "# Rename source and destination addresses to 'id' and create a DataFrame with distinct addresses\n",
    "source_vertices = aggregated_df.select(col(\"`source.address`\").alias(\"id\")).distinct()\n",
    "destination_vertices = aggregated_df.select(col(\"`destination.address`\").alias(\"id\")).distinct()\n",
    "\n",
    "# Combine source and destination vertices into a single DataFrame\n",
    "vertices = source_vertices.union(destination_vertices)\n",
    "\n",
    "graph = GraphFrame(vertices, edges)\n",
    "\n",
    "# Display the vertices and edges\n",
    "# graph.vertices.show()\n",
    "# graph.edges.show()\n",
    "\n",
    "# Use the network.bytes as weights\n",
    "weighted_edges = graph.edges.withColumn(\"weight\", col(\"weight\").cast(\"double\"))\n",
    "\n",
    "# Create a new GraphFrame with weighted edges\n",
    "weighted_graph = GraphFrame(graph.vertices, weighted_edges)\n",
    "# Display the new weighted graph\n",
    "# weighted_graph.vertices.show()\n",
    "# weighted_graph.edges.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d761872-c921-47fc-9070-e4b769a26286",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import math\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.drawing.layout import spring_layout\n",
    "\n",
    "# Convert vertices DataFrame to a list of tuples\n",
    "vertices_df = [(row['id']) for row in vertices.collect()]\n",
    "\n",
    "# Convert edges DataFrame to a list of tuples\n",
    "edges_df = [(row['src'], row['dst'], {'weight': float(row['weight'])}) for row in edges.collect()]\n",
    "print(len(edges_df))\n",
    "\n",
    "# Create a networkx graph\n",
    "nx_graph = nx.DiGraph()\n",
    "\n",
    "# Add vertices\n",
    "nx_graph.add_nodes_from(vertices_df)\n",
    "\n",
    "# Add weighted edges\n",
    "nx_graph.add_edges_from(edges_df)\n",
    "\n",
    "# Compute layout for better visualization\n",
    "#  layout = nx.fruchterman_reingold_layout(nx_graph)\n",
    "layout = nx.spring_layout(nx_graph)\n",
    "\n",
    "# Calculate edge colors based on weights\n",
    "edge_weights = [d['weight'] for (u, v, d) in nx_graph.edges(data=True)]\n",
    "print(len(edge_weights))\n",
    "\n",
    "# Log adjustment\n",
    "log_edge_weights = [math.log(weight + 1) for weight in edge_weights]\n",
    "\n",
    "# Calculate min and max weights after the logarithmic transformation\n",
    "min_log_weight = min(log_edge_weights)\n",
    "max_log_weight = max(log_edge_weights)\n",
    "\n",
    "# Normalize the logarithmic weights\n",
    "normalized_log_weights = [(weight - min_log_weight) / (max_log_weight - min_log_weight) for weight in log_edge_weights]\n",
    "\n",
    "# Rescale the logarithmic weights to a wider range\n",
    "min_rescaled_weight = min(normalized_log_weights)\n",
    "max_rescaled_weight = max(normalized_log_weights)\n",
    "rescaled_weights = [(weight - min_rescaled_weight) / (max_rescaled_weight - min_rescaled_weight) for weight in normalized_log_weights]\n",
    "\n",
    "# Create a color map from white to blue with increasing intensity\n",
    "edge_colors = [(1 - weight, 1 - weight, 1) for weight in rescaled_weights]\n",
    "\n",
    "# Plot the networkx graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "nx.draw(nx_graph, pos=layout, with_labels=True, node_color='skyblue', connectionstyle=f\"arc3,rad=.5\", node_size=300, font_size=8, font_weight='bold', edge_color=edge_colors, width=2, alpha=0.8)\n",
    "\n",
    "# Convert edge weights from bytes to megabytes\n",
    "#edge_labels = nx.get_edge_attributes(nx_graph, 'weight')\n",
    "#edge_labels_mb = {edge: f\"{weight / (1024 * 1024):,.0f} MB\" for edge, weight in edge_labels.items()}\n",
    "\n",
    "# Draw edge labels\n",
    "# nx.draw_networkx_edge_labels(nx_graph, pos=layout, edge_labels=edge_labels_mb, font_size=6, label_pos=0.5, font_color='red')\n",
    "\n",
    "plt.title('Weighted Graph Visualization')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997a8962-bb44-42d7-b4c4-d64acb496251",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
