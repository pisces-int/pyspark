{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42b744b-312e-4e4c-b281-33a7ccbb2325",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1257b185-5428-4a1d-898a-c57c82a9cb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, explode, col, lit,  date_format, to_date, hour, minute\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.sql import functions as F\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access sensitive variables\n",
    "es_url = os.getenv(\"ELASTICSEARCH\")\n",
    "es_username = os.getenv(\"ELASTICSEARCH_USERNAME\")\n",
    "es_password = os.getenv(\"ELASTICSEARCH_PASSWORD\")\n",
    "\n",
    "spark = SparkSession.builder.appName(\"REST_API_with_PySpark_DF\").getOrCreate()\n",
    "\n",
    "# Important parameters can be found here\n",
    "# Some have to be manually modified within the code\n",
    "schema = StructType([\n",
    "    StructField(\"@timestamp\", StringType(), True),\n",
    "    StructField(\"source.address\", StringType(), True),\n",
    "])\n",
    "days_to_fetch = 7  # Specify the number of days to fetch\n",
    "\n",
    "@udf(returnType=ArrayType(schema))\n",
    "def fetch_data(offset: int, limit: int, days: int):\n",
    "    endpoint = \"https://\"+es_url+\"/filebeat-*/_search\"\n",
    "\n",
    "    # Calculate start date based on the specified number of days\n",
    "    import datetime\n",
    "    start_date = (datetime.datetime.now() - datetime.timedelta(days=days)).isoformat()\n",
    "\n",
    "    # Get field names from the schema\n",
    "    fields = [field.name for field in schema.fields]\n",
    "\n",
    "    # Elasticsearch query with time filter, proper pagination, and selected fields\n",
    "    es_query = {\n",
    "        \"size\": limit,\n",
    "        \"from\": offset,\n",
    "        \"_source\": fields,  # Select only specified fields\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\n",
    "                        \"term\": {\n",
    "                            \"suricata.eve.event_type\": \"dns\"\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"term\": {\n",
    "                            \"network.direction\": \"outbound\"\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"range\": {\n",
    "                            \"@timestamp\": {\n",
    "                                \"gte\": start_date\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(endpoint, json=es_query, headers=headers, verify=False, auth=(es_username, es_password))\n",
    "\n",
    "    # Extract hits from the response\n",
    "    hits = response.json().get('hits', {}).get('hits', [])\n",
    "\n",
    "    # Extract necessary fields from hits and create a list of records\n",
    "    records = [{\"@timestamp\": hit.get('_source', {}).get('@timestamp'),\n",
    "                \"source.address\": hit.get('_source', {}).get('source', {}).get('address')}\n",
    "               for hit in hits]\n",
    "\n",
    "    return records  # assuming API returns a list of records\n",
    "\n",
    "# Get total docs\n",
    "total_records = requests.get(\"https://\"+es_url+\"/filebeat-*/_count\", verify=False, auth=(es_username, es_password)).json().get('count', 0)\n",
    "\n",
    "records_per_page = 500\n",
    "\n",
    "# Create DataFrame with pagination information\n",
    "offsets_df = spark.range(0, total_records, records_per_page).select(col(\"id\").alias(\"offset\"), lit(records_per_page).alias(\"limit\"))\n",
    "\n",
    "# Apply fetch_data UDF to get the response with time filter\n",
    "response_df = offsets_df.withColumn(\"response\", explode(fetch_data(\"offset\", \"limit\", lit(days_to_fetch))))\n",
    "\n",
    "# Uncomment the lines above if you want to further explode and select individual fields\n",
    "# response_df.show(truncate=False)\n",
    "\n",
    "# Extract variables using positional indexing\n",
    "result_df = response_df.select(\n",
    "#    \"offset\",\n",
    "#    \"limit\",\n",
    "    col(\"response\")[\"@timestamp\"].alias(\"@timestamp\"),\n",
    "    col(\"response\")[\"source.address\"].alias(\"source.address\")\n",
    ")\n",
    "\n",
    "result_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3963e2-50f7-47c8-a2c8-84e4b024b5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "features_df = result_df.withColumn(\"day\", F.dayofmonth(F.col(\"@timestamp\")))\n",
    "\n",
    "# Calculate the fraction of the day that has passed\n",
    "current_time = F.current_timestamp()\n",
    "fraction_of_day = (F.hour(current_time) * 60 + F.minute(current_time)) / (24 * 60)\n",
    "\n",
    "# Add the fraction_of_day as a new feature\n",
    "features_df = features_df.withColumn(\"fraction_of_day\", lit(fraction_of_day))\n",
    "\n",
    "# Aggregating features by day and source address\n",
    "aggregated_df = features_df.groupBy(\"day\", \"`source.address`\").agg(\n",
    "    F.count(\"*\").alias(\"request_count\"),\n",
    "    F.avg(\"fraction_of_day\").alias(\"avg_fraction_of_day\")\n",
    ")\n",
    "\n",
    "# Filter data for the past 6 days\n",
    "current_day = features_df.select(F.max(\"day\")).first()[0]\n",
    "training_data = aggregated_df.filter((col(\"day\") >= current_day - 6) & (col(\"day\") < current_day + 1))\n",
    "\n",
    "# Machine Learning\n",
    "assembler = VectorAssembler(inputCols=[\"day\", \"request_count\", \"avg_fraction_of_day\"], outputCol=\"features\")\n",
    "regressor = RandomForestRegressor(featuresCol=\"features\", labelCol=\"request_count\", predictionCol=\"prediction\")\n",
    "\n",
    "# Creating a Pipeline\n",
    "pipeline = Pipeline(stages=[assembler, regressor])\n",
    "\n",
    "# Training the model on the past 6 days\n",
    "model = pipeline.fit(training_data)\n",
    "\n",
    "# Predicting on the current day\n",
    "current_day_data = aggregated_df.filter(F.col(\"day\") == current_day).withColumn(\"fraction_of_day\", lit(fraction_of_day))\n",
    "prediction_df = model.transform(current_day_data)\n",
    "\n",
    "# Displaying the actual and predicted values\n",
    "prediction_df.select(\"`source.address`\", \"request_count\", \"prediction\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccabf82-b228-4118-9548-3788938b00a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extracting actual and predicted values\n",
    "source_ips = prediction_df.select(\"`source.address`\").rdd.flatMap(lambda x: x).collect()\n",
    "actual_values = prediction_df.select(\"request_count\").rdd.flatMap(lambda x: x).collect()\n",
    "predicted_values = prediction_df.select(\"prediction\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Perform t-test for paired samples\n",
    "t_statistic, p_value = ttest_rel(actual_values, predicted_values)\n",
    "\n",
    "# Display the t-test results\n",
    "print(f\"T-Statistic: {t_statistic}\")\n",
    "print(f\"P-Value: {p_value}\")\n",
    "\n",
    "# Visualization - Bar plot per source IP\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bar_width = 0.35\n",
    "index = range(len(source_ips))\n",
    "\n",
    "bar1 = ax.bar(index, actual_values, bar_width, label='Actual', color='blue')\n",
    "bar2 = ax.bar(index, predicted_values, bar_width, label='Predicted', color='orange', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Source IP')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Actual vs. Predicted Request Count per Source IP')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(source_ips, rotation=45)\n",
    "ax.legend()\n",
    "\n",
    "# Check if the difference is statistically significant and raise an alert\n",
    "alpha = 0.05\n",
    "for i in range(len(source_ips)):\n",
    "    if actual_values[i] > predicted_values[i] and p_value < alpha:\n",
    "        print(f\"Alert: The difference for Source IP {source_ips[i]} is statistically significant. Actual is above predicted.\")\n",
    "    else:\n",
    "        print(f\"No alert for Source IP {source_ips[i]}\")\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
